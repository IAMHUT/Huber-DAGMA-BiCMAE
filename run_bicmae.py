"""
æ­¥éª¤2: è®­ç»ƒBiCMAEæ¨¡å‹
"""

import os
import numpy as np
import torch
from torch.utils.data import DataLoader
from src.bicmae import BiCMAEDynamicsModel, BiCMAETrainer
from src.utils import (
    simulate_controlled_dynamics,
    TransitionDataset,
    visualize_bicmae_losses,
    visualize_masking_mechanism,
    ensure_dir
)


def main():
    print("=" * 80)
    print("æ­¥éª¤2: è®­ç»ƒ BiCMAE æ¨¡å‹")
    print("=" * 80)

    # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
    ensure_dir('data/figures')
    ensure_dir('data/results')

    # åŠ è½½DAGMAç»“æœ
    print("\n[1] åŠ è½½ DAGMA å› æœå‘ç°ç»“æœ...")
    W_est = np.load('data/results/W_est.npy')
    ancestors = np.load('data/results/ancestors.npy')
    descendants = np.load('data/results/descendants.npy')
    print(f"âœ“ åŠ è½½å› æœå›¾: {W_est.shape}")
    print(f"âœ“ ç¥–å…ˆçŸ©é˜µ: {ancestors.shape}")
    print(f"âœ“ åä»£çŸ©é˜µ: {descendants.shape}")

    # ç”ŸæˆåŠ¨åŠ›å­¦æ•°æ®
    print("\n[2] ç”Ÿæˆå—æ§åŠ¨åŠ›å­¦ç³»ç»Ÿæ•°æ®...")
    n_steps = 5000
    action_dim = 3
    seed = 42

    observations, actions, states = simulate_controlled_dynamics(
        W_est,
        n_steps=n_steps,
        action_dim=action_dim,
        seed=seed
    )
    print(f"âœ“ ç”Ÿæˆ {n_steps} æ­¥è½¬ç§»æ•°æ®")
    print(f"âœ“ è§‚æµ‹ç»´åº¦: {observations.shape[1]}")
    print(f"âœ“ åŠ¨ä½œç»´åº¦: {action_dim}")

    # å‡†å¤‡æ•°æ®é›†
    print("\n[3] å‡†å¤‡è®­ç»ƒæ•°æ®...")
    dataset = TransitionDataset(observations, actions)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )

    batch_size = 128
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    print(f"âœ“ è®­ç»ƒé›†å¤§å°: {train_size}")
    print(f"âœ“ éªŒè¯é›†å¤§å°: {val_size}")
    print(f"âœ“ Batch size: {batch_size}")

    # åˆå§‹åŒ–æ¨¡å‹
    print("\n[4] åˆå§‹åŒ– BiCMAE æ¨¡å‹...")
    device = 'cpu' if torch.cuda.is_available() else 'cuda'
    print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆå§‹åŒ–æ¨¡å‹
    print("\n[4] åˆå§‹åŒ– BiCMAE æ¨¡å‹...")
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device}")

    obs_dim = observations.shape[1]
    latent_dim = obs_dim  # ğŸ”¥ ä¿®æ”¹è¿™é‡Œï¼šä½¿æ½œåœ¨ç»´åº¦ç­‰äºè§‚æµ‹ç»´åº¦

    model = BiCMAEDynamicsModel(
        obs_dim=obs_dim,
        latent_dim=latent_dim,
        action_dim=action_dim,
        ancestors_matrix=ancestors,
        descendants_matrix=descendants,
        enc_hidden=[256, 128],
        fwd_hidden=[128, 128],
        device=device
    )

    print(f"âœ“ è§‚æµ‹ç»´åº¦: {obs_dim}")
    print(f"âœ“ æ½œåœ¨ç»´åº¦: {latent_dim}")
    print(f"âœ“ åŠ¨ä½œç»´åº¦: {action_dim}")

    # è®­ç»ƒæ¨¡å‹
    print("\n[5] è®­ç»ƒ BiCMAE...")
    trainer = BiCMAETrainer(model, device=device, lambda_fwd=1.0, lambda_bwd=1.0)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)

    n_epochs = 200
    loss_history = {
        'total_loss': [],
        'loss_fwd': [],
        'loss_bwd': []
    }

    print(f"âœ“ è®­ç»ƒ {n_epochs} ä¸ª epoch...")

    for epoch in range(n_epochs):
        # è®­ç»ƒ
        train_metrics = trainer.train_epoch(train_loader, optimizer)

        # éªŒè¯
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for batch in val_loader:
                o_t, a_t, o_tp1 = batch
                o_t = o_t.to(device)
                a_t = a_t.to(device)
                o_tp1 = o_tp1.to(device)

                s_tp1_true = model.encode(o_tp1)
                s_tp1_fwd, s_tp1_bwd = model.predict_next_state(o_t, a_t)

                loss = torch.mean((s_tp1_fwd - s_tp1_true) ** 2 +
                                  (s_tp1_bwd - s_tp1_true) ** 2)
                val_loss += loss.item()

        val_loss /= len(val_loader)

        # è®°å½•æŸå¤±
        loss_history['total_loss'].append(train_metrics['total_loss'])
        loss_history['loss_fwd'].append(train_metrics['loss_fwd'])
        loss_history['loss_bwd'].append(train_metrics['loss_bwd'])

        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch + 1}/{n_epochs} - "
                  f"Train Loss: {train_metrics['total_loss']:.6f}, "
                  f"Val Loss: {val_loss:.6f}")

    # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    print("\n[6] ç”Ÿæˆå¯è§†åŒ–...")
    visualize_bicmae_losses(loss_history)
    visualize_masking_mechanism(ancestors, descendants, sample_idx=5)

    # ä¿å­˜æ¨¡å‹
    print("\n[7] ä¿å­˜æ¨¡å‹...")
    torch.save(model.state_dict(), 'data/results/bicmae_model.pth')
    np.save('data/results/loss_history.npy', loss_history)

    print("\n" + "=" * 80)
    print("âœ“ BiCMAE è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)


if __name__ == '__main__':
    main()
